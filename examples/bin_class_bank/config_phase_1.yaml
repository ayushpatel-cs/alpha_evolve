# Configuration for Bank Binary Classification (Playground) evolution
max_iterations: 200
checkpoint_interval: 10
log_level: "INFO"

# LLM configuration
llm:
  primary_model: "google/gemini-2.0-flash-lite-001"
  primary_model_weight: 0.6
  secondary_model: "google/gemini-2.0-flash-001"
  secondary_model_weight: 0.4
  api_base: "https://openrouter.ai/api/v1"
  temperature: 0.6
  top_p: 0.95
  max_tokens: 8192
  timeout: 600

# Compute (hint flags; harmless if ignored by runner)
compute:
  use_gpu: true              # prefer CUDA if available
  cuda_visible_devices: "0"  # which GPU to use; "" to disable
  cpu_threads: -1            # allow sklearn CPU parallelism when GPU not used

# Prompt configuration
prompt:
  system_message: |
    You are an expert ML engineer optimizing models for the Kaggle Playground
    "Binary Classification with a Bank Dataset".

    You will REWRITE only the code inside the EVOLVE block of initial_program.py.

    Objective:
    - Maximize ROC AUC on the probability of the positive class (y=1).
    - The evaluator samples a fresh random validation split each run (often stratified).
    - Do NOT rely on global randomness; if needed, set seeds locally and deterministically.

    Contract you MUST follow in the EVOLVE block:
    - Provide a function:
        def fit_predict(train_df, test_df, train_idx, val_idx) -> dict:
            - train_df: pandas DataFrame with binary target column "y".
            - test_df: pandas DataFrame without "y".
            - train_idx, val_idx: numpy int indices into train_df (row-wise).
            - Train ONLY on train_idx rows; NEVER use val rows (features, imputation stats, CV, or tuning).
            - Return a dict with:
                "val_pred": np.ndarray of probabilities P(y=1) for val_idx (same order/length),
                "test_pred": np.ndarray of probabilities P(y=1) for ALL test rows,
                "model_info": short string (optional).
    - Handle missing values robustly; avoid fragile feature drops.
    - Keep runtime reasonable (target: <10 minutes per evaluation).
    - Prefer sturdy baselines (e.g., tree boosting via scikit-learn, XGBoost/LightGBM if available),
      reliable preprocessing (numeric median impute; categorical most_frequent + OHE), and no leakage.

    Style:
    - Clean, deterministic, portable scikit-learn pipelines are preferred.
    - Use OneHotEncoder(handle_unknown="ignore") for categoricals; impute missing values.
    - Output probabilities in [0,1] for both val and test.
  num_top_programs: 5
  use_template_stochasticity: true

# Database / search configuration
database:
  population_size: 80
  archive_size: 40
  num_islands: 5
  elite_selection_ratio: 0.25
  exploitation_ratio: 0.65

# Evaluator configuration
evaluator:
  timeout: 600
  cascade_evaluation: true
  # For binary ROC AUC; modest thresholds for quick pruning then deeper eval
  cascade_thresholds: [0.97, 0.975]
  parallel_evaluations: 6
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: false
allow_full_rewrites: true
