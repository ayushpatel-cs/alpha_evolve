{"id": "b0517537-d969-4f75-a698-ef4ab294c2de", "code": "# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nimport lightgbm as lgb  # Robust and often better than HistGradientBoosting\n\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: LightGBM (generally faster and often better than HGBR)\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    # LightGBM parameters (tuned)\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=1200,\n        learning_rate=0.02,\n        max_depth=7,\n        num_leaves=60,\n        min_child_samples=25,\n        subsample=0.85,\n        colsample_bytree=0.75,\n        reg_alpha=0.15,\n        reg_lambda=0.15,\n        random_state=42,\n        n_jobs=-1,\n        boosting_type=\"gbdt\",\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)", "language": "python", "parent_id": "91332581-9f35-43ba-9c24-44a38b5182bb", "generation": 4, "timestamp": 1754925969.6469524, "iteration_found": 70, "metrics": {"validity": 1.0, "log_rmse": 0.11470284087454326, "combined_score": 0.8971000730700993, "val_frac": 0.3, "n_train": 420, "n_val": 180, "eval_time": 58.93648838996887, "stage2_passed": 0.0, "timeout": true}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"validity": 0.0, "log_rmse": Infinity, "combined_score": 0.0, "val_frac": 0.3, "n_train": 0, "n_val": 0, "eval_time": 300.17636132240295, "error": "Subprocess timed out after 300s"}, "island": 2}, "artifacts_json": "{\"timeout\": true, \"timeout_duration\": 600, \"failure_stage\": \"stage2\", \"error_type\": \"timeout\", \"stage2_timeout\": true}", "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are an expert ML engineer optimizing models for the Kaggle \"House Prices: Advanced Regression Techniques\" task.\nYou will REWRITE the code only inside the EVOLVE block of initial_program.py.\n\nObjective:\n- Minimize RMSE on log1p(SalePrice) (i.e., Kaggle competition metric).\n- The evaluator samples a fresh random validation split every run.\n- Do NOT rely on global randomness; set seeds only locally if needed.\n\nContract you MUST follow in the EVOLVE block:\n- Provide a function:\n    def fit_predict(train_df, test_df, train_idx, val_idx) -> dict:\n        - train_df: pandas DataFrame with \"SalePrice\" in training.\n        - test_df: pandas DataFrame without \"SalePrice\".\n        - train_idx, val_idx: numpy int indices into train_df (row-wise).\n        - Train ONLY on train_idx rows; NEVER use val rows for fitting or preprocessing stats.\n        - Return a dict with:\n            \"val_pred\": np.ndarray of SalePrice predictions for val_idx (same order/length),\n            \"test_pred\": np.ndarray of SalePrice predictions for ALL test rows,\n            \"model_info\": short string (optional).\n- Fit on log1p(SalePrice) and predict SalePrice (apply expm1 to model outputs).\n- Handle missing values robustly; avoid fragile feature drops.\n- Keep runtime reasonable (sub-10 minutes per evaluation).\n- Prefer sturdy baselines (e.g., Gradient Boosting, XGBoost/LightGBM if available),\n  reliable preprocessing (impute numerics/categoricals), and avoid target leakage.\n\nStyle:\n- Clean, deterministic, portable scikit-learn pipelines are preferred.\n- Use OneHotEncoder(handle_unknown=\"ignore\") for categoricals.\n- Median/most_frequent imputers for safety.\n- Clip negative predictions to 0.\n", "user": "# Current Program Information\n- Current performance metrics: - validity: 0.0000\n- log_rmse: inf\n- combined_score: 0.0000\n- val_frac: 0.3000\n- n_train: 0.0000\n- n_val: 0.0000\n- eval_time: 300.1764\n- error: Subprocess timed out after 300s\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: log_rmse, val_frac, eval_time. Consider continuing with similar changes.\n- Metrics showing regression: validity, combined_score, n_train, n_val. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: validity: 1.0000, log_rmse: 0.1233, combined_score: 0.8903, val_frac: 0.2000, n_train: 1168.0000, n_val: 292.0000, eval_time: 2.4808\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: validity: 1.0000, log_rmse: 0.1203, combined_score: 0.8926, val_frac: 0.2000, n_train: 1168.0000, n_val: 292.0000, eval_time: 195.1317\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: validity: 1.0000, log_rmse: 0.1167, combined_score: 0.8955, val_frac: 0.2000, n_train: 1168.0000, n_val: 292.0000, eval_time: 2.1704\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 209.1975)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor  # Use XGBoost for potentially better performance\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: XGBRegressor\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    # Use XGBoost instead of Ridge or HGBR\n    model = XGBRegressor(\n        objective='reg:squarederror',  # Specify objective for regression\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=5,\n        subsample=0.7,\n        colsample_bytree=0.7,\n        random_state=42,\n        n_jobs=-1,  # Use all available cores\n        reg_alpha=0.1,  # L1 regularization\n        reg_lambda=0.1 # L2 regularization\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nKey features: Performs well on validity (1.0000), Performs well on log_rmse (0.1167), Performs well on combined_score (0.8955), Performs well on val_frac (0.2000), Performs well on n_train (1168.0000), Performs well on n_val (292.0000), Performs well on eval_time (2.1704)\n\n\n### Program 2 (Score: 236.7635)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import RobustScaler\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: HistGradientBoostingRegressor (handles large cardinality well after OHE; fast)\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"robust_scaler\", RobustScaler()),\n            (\"quantile\", QuantileTransformer(output_distribution='normal', random_state=42)),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    model = HistGradientBoostingRegressor(\n        loss=\"squared_error\",\n        learning_rate=0.04,\n        max_depth=6,\n        max_iter=700,\n        l2_regularization=0.015,\n        early_stopping=True,\n        validation_fraction=0.12,\n        random_state=42,\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nKey features: Performs well on validity (1.0000), Performs well on log_rmse (0.1203), Performs well on combined_score (0.8926), Performs well on val_frac (0.2000), Performs well on n_train (1168.0000), Performs well on n_val (292.0000), Performs well on eval_time (195.1317)\n\n\n### Program 3 (Score: 209.2420)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: HistGradientBoostingRegressor (handles large cardinality well after OHE; fast)\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\", # Changed from drop to passthrough\n        n_jobs=None,  # keep simple/portable\n    )\n\n    model = HistGradientBoostingRegressor(\n        loss=\"squared_error\",\n        learning_rate=0.05,\n        max_depth=5,\n        max_iter=600,\n        l2_regularization=0.01,\n        early_stopping=True,\n        validation_fraction=0.1,\n        random_state=42,\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nKey features: Performs well on validity (1.0000), Performs well on log_rmse (0.1233), Performs well on combined_score (0.8903), Performs well on val_frac (0.2000), Performs well on n_train (1168.0000), Performs well on n_val (292.0000), Performs well on eval_time (2.4808)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 237.8830)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import RobustScaler\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: HistGradientBoostingRegressor (handles large cardinality well after OHE; fast)\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"robust_scaler\", RobustScaler()),\n            (\"quantile\", QuantileTransformer(output_distribution='normal', random_state=42)),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    model = HistGradientBoostingRegressor(\n        loss=\"squared_error\",\n        learning_rate=0.04,\n        max_depth=6,\n        max_iter=700,\n        l2_regularization=0.015,\n        early_stopping=True,\n        validation_fraction=0.12,\n        random_state=42,\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nKey features: Alternative approach to validity, Alternative approach to log_rmse\n\n\n### Program D2 (Score: 242.0268)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor  # Use XGBoost for potentially better performance\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: XGBRegressor\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    model = XGBRegressor(\n        objective='reg:squarederror',  # Specify the objective function\n        n_estimators=1000,  # Number of boosting rounds\n        learning_rate=0.03,  # Step size shrinkage\n        max_depth=6,  # Maximum depth of a tree\n        subsample=0.7,  # Subsample ratio of the training instance\n        colsample_bytree=0.7,  # Subsample ratio of columns when constructing each tree\n        random_state=42,  # Random number seed for reproducibility\n        n_jobs=None,\n        reg_alpha=0.01,\n        reg_lambda=0.01,\n        gamma=0.1\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nKey features: Alternative approach to validity, Alternative approach to log_rmse\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 209.1975, Type: High-Performer)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor  # Use XGBoost for potentially better performance\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: XGBRegressor\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    # Use XGBoost instead of Ridge or HGBR\n    model = XGBRegressor(\n        objective='reg:squarederror',  # Specify objective for regression\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=5,\n        subsample=0.7,\n        colsample_bytree=0.7,\n        random_state=42,\n        n_jobs=-1,  # Use all available cores\n        reg_alpha=0.1,  # L1 regularization\n        reg_lambda=0.1 # L2 regularization\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nUnique approach: Modification: Full rewrite, Excellent validity (1.000), Alternative log_rmse approach\n\n\n### Inspiration 2 (Score: 242.0268, Type: High-Performer)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor  # Use XGBoost for potentially better performance\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: XGBRegressor\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    model = XGBRegressor(\n        objective='reg:squarederror',  # Specify the objective function\n        n_estimators=1000,  # Number of boosting rounds\n        learning_rate=0.03,  # Step size shrinkage\n        max_depth=6,  # Maximum depth of a tree\n        subsample=0.7,  # Subsample ratio of the training instance\n        colsample_bytree=0.7,  # Subsample ratio of columns when constructing each tree\n        random_state=42,  # Random number seed for reproducibility\n        n_jobs=None,\n        reg_alpha=0.01,\n        reg_lambda=0.01,\n        gamma=0.1\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nUnique approach: Modification: Full rewrite, Excellent validity (1.000), Alternative log_rmse approach\n\n\n### Inspiration 3 (Score: 237.8830, Type: High-Performer)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import RobustScaler\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: HistGradientBoostingRegressor (handles large cardinality well after OHE; fast)\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"robust_scaler\", RobustScaler()),\n            (\"quantile\", QuantileTransformer(output_distribution='normal', random_state=42)),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    model = HistGradientBoostingRegressor(\n        loss=\"squared_error\",\n        learning_rate=0.04,\n        max_depth=6,\n        max_iter=700,\n        l2_regularization=0.015,\n        early_stopping=True,\n        validation_fraction=0.12,\n        random_state=42,\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nUnique approach: Modification: Full rewrite, Excellent validity (1.000), Alternative log_rmse approach\n\n\n### Inspiration 4 (Score: inf, Type: High-Performer)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom lightgbm import LGBMRegressor  # Use LightGBM for potentially better performance\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: LGBMRegressor\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    # Use LightGBM instead of Ridge or HGBR\n    model = LGBMRegressor(\n        objective='regression',  # Specify objective for regression\n        n_estimators=700,\n        learning_rate=0.03,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1,  # Use all available cores\n        reg_alpha=0.1,  # L1 regularization\n        reg_lambda=0.1, # L2 regularization\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nUnique approach: Modification: Full rewrite, Alternative validity approach, Excellent log_rmse (inf)\n\n\n### Inspiration 5 (Score: 232.6137, Type: High-Performer)\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\n\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: HistGradientBoostingRegressor (handles large cardinality well after OHE; fast)\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    model = HistGradientBoostingRegressor(\n        loss=\"squared_error\",\n        learning_rate=0.04,\n        max_depth=6,\n        max_iter=700,\n        l2_regularization=0.015,\n        early_stopping=True,\n        validation_fraction=0.12,\n        random_state=42,\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\nUnique approach: Modification: Full rewrite, Excellent validity (1.000), Alternative log_rmse approach\n\n\n\n# Current Program\n```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nimport lightgbm as lgb  # Robust and often better than HistGradientBoosting\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling, quantile transform\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: LightGBM (generally faster and often better than HGBR)\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n            (\"quantile\", QuantileTransformer(output_distribution='normal')),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    # LightGBM parameters (tuned)\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=1000,\n        learning_rate=0.02,\n        max_depth=7,\n        num_leaves=70,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["```python\n# initial_program.py\n\"\"\"\nInitial program for Kaggle House Prices (Ames) using OpenEvolve.\n\nThe evaluator will import this file and call `fit_predict(train_df, test_df, train_idx, val_idx)`\nto get validation predictions (for scoring) and test predictions (for convenience).\n\nOnly the code between the EVOLVE markers will be edited by OpenEvolve.\nEverything else should remain stable so the evaluator can call it reliably.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport pandas as pd\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolvable modeling code for the House Prices task.\n\nContract:\n    def fit_predict(train_df, test_df, train_idx, val_idx):\n        Inputs:\n            - train_df: pandas.DataFrame containing the full training set with a \"SalePrice\" column\n            - test_df:  pandas.DataFrame containing the Kaggle test set (no \"SalePrice\")\n            - train_idx: np.ndarray or list of row indices for training subset (within train_df index order)\n            - val_idx:   np.ndarray or list of row indices for validation subset (within train_df index order)\n\n        Returns:\n            dict with keys:\n                - \"val_pred\": 1D np.ndarray of predictions for validation rows (aligned with val_idx)\n                - \"test_pred\": 1D np.ndarray of predictions for the test set (same length as test_df)\n                - \"model_info\": optional string for debugging\n\nNotes:\n  - Score metric is RMSE on log1p(SalePrice), so we train on log1p targets and inverse-transform with expm1.\n  - Keep it reasonably fast and robust; evaluator may call many times with different splits.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nimport lightgbm as lgb  # Robust and often better than HistGradientBoosting\n\n\ndef _prepare_features(\n    df: pd.DataFrame, is_train: bool = True\n) -> Tuple[pd.DataFrame, np.ndarray | None]:\n    \"\"\"\n    Basic cleaning:\n      - Leave all original columns except SalePrice for features.\n      - Don't drop columns aggressively; impute instead.\n      - Return (X_df, y) with y = log1p(SalePrice) if training.\n    \"\"\"\n    df = df.copy()\n\n    y = None\n    if is_train:\n        if \"SalePrice\" not in df.columns:\n            raise ValueError(\"train_df must contain 'SalePrice'\")\n        # Target transform for Kaggle metric (RMSE on log targets)\n        y = np.log1p(df[\"SalePrice\"].values.astype(float))\n        df = df.drop(columns=[\"SalePrice\"])\n\n    # Keep Id around if present; model won\u2019t use it (we\u2019ll drop explicitly)\n    # Avoid leakage by not adding engineered features from target.\n\n    return df, y\n\n\ndef _build_pipeline(X_df: pd.DataFrame) -> Pipeline:\n    \"\"\"\n    Build a reasonably strong, fast baseline:\n      - Numeric: median impute, scaling\n      - Categorical: most_frequent impute + OneHotEncoder(handle_unknown='ignore')\n      - Model: LightGBM (generally faster and often better than HGBR)\n    \"\"\"\n    # Identify columns by dtype (robust if categories come in as object)\n    numeric_features = X_df.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features = [c for c in X_df.columns if c not in numeric_features]\n\n    # Drop purely non-informative identifiers if present\n    for ident in [\"Id\"]:\n        if ident in numeric_features:\n            numeric_features = [c for c in numeric_features if c != ident]\n        if ident in categorical_features:\n            categorical_features = [c for c in categorical_features if c != ident]\n\n    numeric_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    categorical_tf = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_features),\n            (\"cat\", categorical_tf, categorical_features),\n        ],\n        remainder=\"passthrough\",\n        n_jobs=None,  # keep simple/portable\n    )\n\n    # LightGBM parameters (tuned)\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=1200,\n        learning_rate=0.02,\n        max_depth=7,\n        num_leaves=60,\n        min_child_samples=25,\n        subsample=0.85,\n        colsample_bytree=0.75,\n        reg_alpha=0.15,\n        reg_lambda=0.15,\n        random_state=42,\n        n_jobs=-1,\n        boosting_type=\"gbdt\",\n    )\n\n    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", model)])\n    return pipe\n\n\ndef fit_predict(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    train_idx: np.ndarray,\n    val_idx: np.ndarray,\n) -> Dict[str, Any]:\n    \"\"\"\n    Train on train_idx and predict on val_idx and the entire test_df.\n\n    Returns:\n        {\n          \"val_pred\": np.ndarray of predicted SalePrice for validation rows (aligned with val_idx),\n          \"test_pred\": np.ndarray of predicted SalePrice for all rows in test_df,\n          \"model_info\": str (optional)\n        }\n    \"\"\"\n    # Split & prepare\n    X_all, y_all = _prepare_features(train_df, is_train=True)\n    X_test, _ = _prepare_features(test_df, is_train=False)\n\n    # Build pipeline fit on train subset\n    pipe = _build_pipeline(X_all)\n\n    X_tr = X_all.iloc[train_idx]\n    y_tr = y_all[train_idx]\n    X_va = X_all.iloc[val_idx]\n\n    pipe.fit(X_tr, y_tr)\n\n    # Predict log targets, invert with expm1, clip to non-negative\n    val_log_pred = pipe.predict(X_va)\n    test_log_pred = pipe.predict(X_test)\n\n    val_pred = np.expm1(val_log_pred).astype(float)\n    test_pred = np.expm1(test_log_pred).astype(float)\n\n    # Safety: ensure no negatives due to numerical noise\n    val_pred = np.clip(val_pred, a_min=0.0, a_max=None)\n    test_pred = np.clip(test_pred, a_min=0.0, a_max=None)\n\n    info = f\"pipe={pipe.__class__.__name__}, model={pipe.named_steps['model'].__class__.__name__}\"\n\n    return {\n        \"val_pred\": val_pred,\n        \"test_pred\": test_pred,\n        \"model_info\": info,\n    }\n# EVOLVE-BLOCK-END\n\n\n# ---- Fixed API below (do not evolve) ----\n\ndef run_houseprice(random_state: int = 0, val_frac: float = 0.2) -> dict:\n    \"\"\"\n    Convenience runner to test the pipeline locally (not used by evaluator in scoring).\n    Trains on a random split and prints a quick score.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    idx = np.arange(len(train_df))\n    train_idx, val_idx = train_test_split(idx, test_size=val_frac, random_state=random_state, shuffle=True)\n\n    out = fit_predict(train_df, test_df, train_idx, val_idx)\n\n    # Compute RMSE on log1p for quick feedback\n    y_true = np.log1p(train_df.loc[val_idx, \"SalePrice\"].values.astype(float))\n    y_pred = np.log1p(out[\"val_pred\"])\n    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\n    print(f\"[Local run] log-RMSE={rmse:.5f} | {out.get('model_info','')}\")\n    return {\"rmse\": rmse, **out}\n\n\nif __name__ == \"__main__\":\n    run_houseprice(random_state=0, val_frac=0.2)\n```"]}}}